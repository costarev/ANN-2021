# Практическое задание №4, вариант 7

## Задание
Необходимо реализовать нейронную сеть вычисляющую результат заданной логической операции. Затем реализовать функции, которые будут симулировать работу построенной модели. Функции должны принимать тензор входных данных и список весов. Должно быть реализовано 2 функции:

Функция, в которой все операции реализованы как поэлементные операции над тензорами
Функция, в которой все операции реализованы с использованием операций над тензорами из NumPy
Для проверки корректности работы функций необходимо:

Инициализировать модель и получить из нее веса (Как получить веса слоя, Как получить список слоев модели)
Прогнать датасет через не обученную модель и реализованные 2 функции. Сравнить результат.
Обучить модель и получить веса после обучения
Прогнать датасет через обученную модель и реализованные 2 функции. Сравнить результат.

## Реализация

В качестве модели была выбрана ИНС со следующей структурой:
* Слой - Relu, 16 нейронов `(input_shape = (3,))`
* Слой - Relu, 32 нейрона
* Слой - Sigmoid, 1 нейрон

Параметры обучения сети:
* Оптимизатор: adam
* Функция потерь: бинарная кросс-энтропия (binary_crossentropy)
* Метрика: точность (accuracy)
* Кол-во эпох: 100
* Размер партии (batch_size): 1

Для симуляции работы нейронной сети на основе весов были реализованы две функции:
* `simulation` - операции над тензорами реализованы без использования numpy
* `numpy_simulation` - операции над тензорами реализованы с помощью numpy

В обеих функциях реализована формула `output = relu/sigmoid(dot(W, input) + b)`, которая применяется поочередно для всех нейронов всех слоев, тем самым симулируя работу нейронной сети.

Функция `model_prediction ` отвечает за получение весов из модели, вызов функций `simulation`, `numpy_simulation` и `model.predict` самой модели, а также за запись в файл результатов тестирования.

Также реализованы функции:
 * `expression`  для получения правильных ответов по входным данным
 * `sigmoid` для реализации функции sigmoid
 * `relu` для реализации функции relu

## Ввод и вывод
Данные подаются на вход из файла data.csv, где должны содержаться строки в формате:
* `a, b, c`

Выходные данные программы записываются в файл output.txt. Записывается результат до обучения и после обучения (100 эпох) модели.

## Тестирование

Входные данные (data.csv):
```
0, 0, 0
0, 0, 1
0, 1, 0
0, 1, 1
1, 0, 0
1, 0, 1
1, 1, 0
1, 1, 1
```
Вывод (output.txt):
```
NO TRAINING

MODEL PREDICT
[[0.5       ]
 [0.46929803]
 [0.47456217]
 [0.43753773]
 [0.4461317 ]
 [0.41832662]
 [0.42128155]
 [0.39476448]]
NUMPY SIMULATION
[[0.5       ]
 [0.46929805]
 [0.47456217]
 [0.43753773]
 [0.4461317 ]
 [0.41832662]
 [0.42128154]
 [0.39476448]]
SIMULATION
[[0.5       ]
 [0.46929805]
 [0.47456217]
 [0.43753773]
 [0.4461317 ]
 [0.41832662]
 [0.42128154]
 [0.39476448]]

AFTER 100 EPOCHS

MODEL PREDICT
[[0.06713432]
 [0.01746669]
 [0.01907349]
 [0.9592269 ]
 [0.91904044]
 [0.03694353]
 [0.0261873 ]
 [0.9680423 ]]
NUMPY SIMULATION
[[0.06713434]
 [0.01746668]
 [0.01907345]
 [0.95922689]
 [0.91904048]
 [0.03694348]
 [0.0261873 ]
 [0.96804231]]
SIMULATION
[[0.06713434]
 [0.01746668]
 [0.01907345]
 [0.95922689]
 [0.91904048]
 [0.03694348]
 [0.0261873 ]
 [0.96804231]]
```

* Из результатов видно, что как до обучения, так и после обучения значения, полученные симуляциями и реальные практически совпадают.
* Значения, полученные симуляциями с помощью numpy и без него полностью совпадают.
* Имеются некоторые различия на уровне `+-0.00000005`. Их можно объяснить потерей точности при операциях над числами с плавающей запятой, либо с округлением. Пример:
```
MODEL PREDICT
[...
[0.03694353]
...]
NUMPY SIMULATION
[...
 [0.03694348]
...]
```

* В целом, погрешности такого уровня практически не имеют влияния на результат. 

Можно сделать вывод, что поведение нейронной сети при известных весах можно смоделировать с помощью сочетания операций над тензорами. Более удобно это делать с применением numpy, при этом результат остается идентичным.
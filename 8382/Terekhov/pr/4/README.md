# Практика №4, вариант 2

## Задание

Необходимо реализовать нейронную сеть вычисляющую результат заданной логической операции. 

`(a or b) xor not(b and c)`

Затем реализовать функции, которые будут симулировать работу построенной модели. 
Функции должны принимать тензор входных данных и список весов.
Должно быть реализовано 2 функции:

1. Функция, в которой все операции реализованы как поэлементные операции над тензорами
2. Функция, в которой все операции реализованы с использованием операций над тензорами из NumPy

Для проверки корректности работы функций необходимо:

1. Инициализировать модель и получить из нее веса
2. Прогнать датасет через не обученную модель и реализованные 2 функции. Сравнить результат.
3. Обучить модель и получить веса после обучения
4. Прогнать датасет через обученную модель и реализованные 2 функции. Сравнить результат.

## Выполнение работы

Для работы были использованы функции из материалов занятия, а также написаны несколько новых.

Для перемножения матриц была написана специальная функция.
```python
def naive_matrix_matrix_dot(x, y):
    assert len(x.shape) == 2 # размерность первого тензора 2
    assert len(y.shape) == 2 # размерность второго тензора 2
    assert x.shape[1] == y.shape[0] # проверка возможности перемножения тензоров
    z = [[0 for _ in range(y.shape[1])] for _ in range(x.shape[0])]
    for i in range(x.shape[0]):
        for j in range(y.shape[1]):
            cur_row = x[i, :]
            cur_col = y[:, j]
            z[i][j] = naive_vector_dot(cur_row, cur_col)
    return np.array(z)
```

Также была реализована функция вычисляющая значение сигмоиды.

```python
def naive_sigmoid(x):
    y = []
    for x_i in x:
        y.append([1 / (1 + math.exp(-x_i))])
    return y
```

Numpy в данных функциях использовался только для совместимости с кодом из материалов занятия.

Исходный датасет (в комментариях ожидаемое значение):

```python
    X = np.array([
        [0, 0, 0],  # 1
        [0, 0, 1],  # 1
        [0, 1, 0],  # 0
        [0, 1, 1],  # 1
        [1, 0, 0],  # 0
        [1, 0, 1],  # 0
        [1, 1, 0],  # 0
        [1, 1, 1],  # 1
    ])
```

Чтобы построить модель была написана функция:

```python
def build_model():
    model = Sequential()
    model.add(Dense(32, activation='relu', input_shape=(3,)))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    return model
```

Данная модель имеет 3 скрытых слоя с функцией активации relu и один выходной сигмоидальный слой.

Функция, симулирующая работу модели, в которой все операции реализованы как поэлементные операции над тензорами:

```python
def naive_sim(layers, data):
    for layer in layers[:-1]:
        weights = layer.get_weights()[0]
        biases = layer.get_weights()[1]
        data = naive_relu(naive_add_matrix_and_vector(naive_matrix_matrix_dot(data, weights), biases))
    weights = layers[-1].get_weights()[0]
    biases = layers[-1].get_weights()[1]
    data = naive_sigmoid(naive_add_matrix_and_vector(naive_matrix_matrix_dot(data, weights), biases))
    return data
```

Функция, симулирующая работу модели, в которой все операции реализованы с использованием операций над тензорами из NumPy:

```python
def np_sim(layers, data):
    for layer in layers[:-1]:
        weights = layer.get_weights()[0]
        biases = layer.get_weights()[1]
        data = np.maximum(np.dot(data, weights) + biases, 0)
    weights = layers[-1].get_weights()[0]
    biases = layers[-1].get_weights()[1]
    data = 1 / (1 + np.exp(-(np.dot(data, weights) + biases)))
    return data
```

Параметры обучения модели:

```python
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X, y, epochs=64, batch_size=4, verbose=False)
```

## Пример работы программы

В ходе работы программы выводятся данные до обучения:

```
Before fitting
Keras	[0.5]	[0.5037842]	[0.5139593]	[0.5198312]	[0.5223046]	[0.5083664]	[0.5028926]	[0.5166496]
Naive	[0.5]	[0.5037842]	[0.5139593]	[0.5198312]	[0.5223047]	[0.5083665]	[0.5028926]	[0.5166496]
Numpy	[0.5]	[0.5037842]	[0.5139593]	[0.5198312]	[0.5223047]	[0.5083665]	[0.5028926]	[0.5166496]
```

В первой строке представлены результаты прогона датасета через необученную модель.
Во второй -- предсказания полученные наивной симуляцией.
В третьей -- предсказания полученные симуляцией, использующей numpy для вычислений. 

Также выводятся результаты после округления:

```
After round
Keras	[0.]	[1.]	[1.]	[1.]	[1.]	[1.]	[1.]	[1.]
Naive	[0.]	[1.]	[1.]	[1.]	[1.]	[1.]	[1.]	[1.]
Numpy	[0.]	[1.]	[1.]	[1.]	[1.]	[1.]	[1.]	[1.]
```

Затем происходит обучение модели, и выполняются аналогичные действия, плюс реальные значения заданной логической функции.

```
After fitting
Keras	[0.5662422]	[0.7179619]	[0.4793868]	[0.7672746]	[0.4919746]	[0.4924762]	[0.4316228]	[0.7487054]
Naive	[0.5662422]	[0.7179619]	[0.4793868]	[0.7672746]	[0.4919746]	[0.4924763]	[0.4316228]	[0.7487054]
Numpy	[0.5662422]	[0.7179619]	[0.4793868]	[0.7672746]	[0.4919746]	[0.4924763]	[0.4316228]	[0.7487054]
After round
Keras	[1.]	[1.]	[0.]	[1.]	[0.]	[0.]	[0.]	[1.]
Naive	[1.]	[1.]	[0.]	[1.]	[0.]	[0.]	[0.]	[1.]
Numpy	[1.]	[1.]	[0.]	[1.]	[0.]	[0.]	[0.]	[1.]
Real	True	True	False	True	False	False	False	True
```
# Практическое задание 4
Ларин Антон  
Гр. 8383
  
## Условие задачи

> Вариант 4 

Необходимо реализовать нейронную сеть вычисляющую результат заданной логической операции. Затем реализовать функции, которые будут симулировать работу построенной модели. Функции должны принимать тензор входных данных и список весов. Должно быть реализовано 2 функции:
1. Функция, в которой все операции реализованы как поэлементные операции над тензорами 
2. Функция, в которой все операции реализованы с использованием операций над тензорами из NumPy  

Для проверки корректности работы функций необходимо:
1. Инициализировать модель и получить из нее веса ([Как получить веса слоя](https://www.google.com/url?q=https://keras.io/layers/about-keras-layers/&sa=D&source=editors&ust=1616676963854000&usg=AOvVaw3q0cgOtIxBoThjo2nYDmjm), [Как получить список слоев модели](https://www.google.com/url?q=https://keras.io/models/about-keras-models/&sa=D&source=editors&ust=1616676963855000&usg=AOvVaw3jmqHBPBCDCEWbVHrVmbcb))
2. Прогнать датасет через не обученную модель и реализованные 2 функции. Сравнить результат.
3. Обучить модель и получить веса после обучения
4. Прогнать датасет через обученную модель и реализованные 2 функции. Сравнить результат.

*Примечание: так как множество всех наблюдений ограничен, то обучение проводить можно на всем датасете без контроля.*

#### Вариант 4
(a or b) and (b or c)

## Выполнение работы

Модель имеет следующий вид:
```python
def get_model()->Sequential:
    model = Sequential()
    model.add(Dense(16, activation='relu', input_shape=(3,)))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model
```

  
Для для оценки модель можно вызвать как функцию:
```python
model(data)
```
Для прогона данных по модели при помощи NumPy реализована функция `def exec_numpy(layers,activations,data:np.ndarray)`  
В ней для каждого слоя производится векторное перемножение данными с весами и сложение со смещением при помощи NumPy, а затем применяется функция активации слоя взятая напрямую из модели
```python
state = activations[i](np.dot(state,np.array(layers[i][0]))+layers[i][1])
```
Для прогона данных по модели вручную без использования операций NumPy все операции были переписаны при помощи циклов  
Функции активации:
```python
def naive_relu(x) # Применение функции автивации relu к 'x' поэлементно
```
```python
def naive_sigmoid(x) # Применение функции автивации sigmoid к 'x' поэлементно
```
Операции над тензорами:  
```python
def naive_add(x, y) # Сложение
```
```python
def naive_vector_dot(x, y) # Произведение векторов
```
```python
def naive_dot(x:np.ndarray,y:np.ndarray) # Произведение тензоров(использует naive_vector_dot)
```

Реализована функция для оценки модели
```python
def exec_naive(layers,data:np.ndarray)
```
В ней функции активации помещаются массив вручную
```python
activations = [naive_relu,naive_relu, naive_sigmoid]
```

И для каждго слоя производятся те же операции, что и в exec_numpy, но используя только naive-реализации функций
```python
state = activations[i](naive_add(naive_dot(state, (layers[i][0])), layers[i][1]))
```

Все три метода используются один раз для необученой модели.  
Затем модель обучается на 100 эпохах и оценка модели повторяется

## Полученные результаты
Вывод программы:  
```
BEFORE FIT:
Execute model
tf.Tensor(
[[0.5       ]
 [0.49358967]
 [0.5392387 ]
 [0.5267958 ]
 [0.5697721 ]
 [0.47204062]
 [0.57235444]
 [0.54083896]], shape=(8, 1), dtype=float32)
Execute with numpy
tf.Tensor(
[[0.5       ]
 [0.49358967]
 [0.5392387 ]
 [0.5267958 ]
 [0.5697721 ]
 [0.47204065]
 [0.57235444]
 [0.54083896]], shape=(8, 1), dtype=float32)
Execute with naive
[[0.5       ]
 [0.49358967]
 [0.5392387 ]
 [0.52679583]
 [0.56977212]
 [0.47204063]
 [0.57235443]
 [0.54083895]]
2021-03-25 17:29:58.203345: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 17:29:58.203733: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2799925000 Hz

AFTER FIT(100 epochs)
Execute model
tf.Tensor(
[[0.0487119 ]
 [0.07120314]
 [0.9823856 ]
 [0.9972477 ]
 [0.05707517]
 [0.9266919 ]
 [0.99809974]
 [0.99973214]], shape=(8, 1), dtype=float32)
Execute with numpy
tf.Tensor(
[[0.0487119 ]
 [0.07120314]
 [0.9823856 ]
 [0.9972477 ]
 [0.05707517]
 [0.9266919 ]
 [0.99809974]
 [0.99973214]], shape=(8, 1), dtype=float32)
Execute with naive
[[0.04871195]
 [0.07120316]
 [0.98238556]
 [0.99724774]
 [0.05707521]
 [0.92669194]
 [0.9980997 ]
 [0.99973215]]
```

 - До обучения модели все выходы близки к 0.5, т.е. аналогичны броску монетки.
 - После обучения модель дает верные ответы с высокой точностью (ошибка не превышает 0.1)
 - Результаты работы всех трех методов оценки модели (`model(data)`, `exec_numpy`, `exec_naive`) совпадают с точностью до седьмого знака после запятой.